{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.1\n",
    "\n",
    "Implement a deep convolutional neural network from scratch using a\n",
    "popular deep learning framework (e.g., TensorFlow or PyTorch). Train\n",
    "and evaluate the network on a standard image classification dataset,\n",
    "such as CIFAR-10 or MNIST. First, experiment blindly with various\n",
    "hyperparameters and architectures and observe the model’s\n",
    "performance. Second, apply the measurements proposed in this book\n",
    "to reduce the hyperparameter search space and observe the model’s\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'train_batch_size': 64,\n",
    "    'test_batch_size': 1000,\n",
    "    'epochs': 25,\n",
    "    'lr': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/pytorch/examples/tree/main/mnist\n",
    "\n",
<<<<<<< HEAD
=======
    "# What josh did: G is the compression ratio between before and after a layer. He tried to increase the compression ratio \n",
    "\n",
    "\n",
>>>>>>> 1a41d11d085a5ba90ad56c0c87d406dcc75f63b9
    "# There are 60,000 training images and 10,000 test images, each of size 28x28 pixels\n",
    "train_dataset = datasets.MNIST(root='./mnist', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hyperparameters['train_batch_size'], shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=hyperparameters['test_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64*7*7, 10)\n",
    "    \n",
    "    def forward(self, x): # (batch_size, channels, height, width)\n",
    "        G_conv_1 = x.flatten().size(dim=0)\n",
    "        x = self.conv_1(x) # [64, 1, 28, 28] -> [64, 32, 28, 28]\n",
    "        G_conv_1 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        G_max_pool_1 = x.flatten().size(dim=0)\n",
    "        x = F.max_pool2d(x, kernel_size=(2, 2)) # [64, 32, 28, 28] -> [64, 32, 14, 14]\n",
    "        G_max_pool_1 /= x.flatten().size(dim=0)\n",
    "\n",
    "        G_conv2 = x.flatten().size(dim=0)\n",
    "        x = self.conv_2(x) # [64, 32, 14, 14] -> [64, 64, 14, 14]\n",
    "        G_conv2 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        G_max_pool_2 = x.flatten().size(dim=0)\n",
    "        x = F.max_pool2d(x, kernel_size=(2, 2)) # [64, 64, 14, 14] -> [64, 64, 7, 7]\n",
    "        G_max_pool_2 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = x.view(-1, 64*7*7) # [64, 64, 7, 7] -> [64, 64*7*7]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # print(\"-------------------\")\n",
    "        # print(f\"G_conv_1: {G_conv_1}\")\n",
    "        # print(f\"G_max_pool_1: {G_max_pool_1}\")\n",
    "        # print(f\"G_conv2: {G_conv2}\")\n",
    "        # print(f\"G_max_pool_2: {G_max_pool_2}\")\n",
    "        # print(\"-------------------\")\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data, target) in enumerate(tepoch):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            tepoch.set_description(f\"Train Epoch: {epoch}\")\n",
    "            \n",
    "def test(m, device, test_loader):\n",
    "    m.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = m(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1: 100%|██████████| 938/938 [00:24<00:00, 38.70batch/s, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3701, Accuracy: 8830/10000 (88%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2: 100%|██████████| 938/938 [00:23<00:00, 39.46batch/s, loss=0.0675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1895, Accuracy: 9456/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3: 100%|██████████| 938/938 [00:23<00:00, 39.20batch/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1660, Accuracy: 9506/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4: 100%|██████████| 938/938 [00:23<00:00, 39.35batch/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1220, Accuracy: 9630/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5: 100%|██████████| 938/938 [00:24<00:00, 38.93batch/s, loss=0.0702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 9660/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6: 100%|██████████| 938/938 [00:25<00:00, 36.87batch/s, loss=0.0998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0848, Accuracy: 9741/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7: 100%|██████████| 938/938 [00:23<00:00, 39.29batch/s, loss=0.161]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0796, Accuracy: 9743/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8: 100%|██████████| 938/938 [00:24<00:00, 37.90batch/s, loss=0.0148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0661, Accuracy: 9791/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9: 100%|██████████| 938/938 [00:25<00:00, 37.25batch/s, loss=0.0412] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0625, Accuracy: 9799/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10: 100%|██████████| 938/938 [00:26<00:00, 35.26batch/s, loss=0.0562] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 9812/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11: 100%|██████████| 938/938 [00:26<00:00, 35.65batch/s, loss=0.181]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 9814/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12: 100%|██████████| 938/938 [00:24<00:00, 38.20batch/s, loss=0.0229] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0578, Accuracy: 9803/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13: 100%|██████████| 938/938 [00:25<00:00, 37.21batch/s, loss=0.128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0607, Accuracy: 9796/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14: 100%|██████████| 938/938 [00:24<00:00, 37.75batch/s, loss=0.0679] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0478, Accuracy: 9831/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15: 100%|██████████| 938/938 [00:24<00:00, 38.16batch/s, loss=0.0237] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0467, Accuracy: 9844/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16: 100%|██████████| 938/938 [00:24<00:00, 38.77batch/s, loss=0.0407] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9836/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17: 100%|██████████| 938/938 [00:24<00:00, 38.61batch/s, loss=0.00401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9850/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18: 100%|██████████| 938/938 [00:24<00:00, 37.62batch/s, loss=0.0151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0420, Accuracy: 9856/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19: 100%|██████████| 938/938 [00:24<00:00, 38.12batch/s, loss=0.134]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 9837/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20: 100%|██████████| 938/938 [00:24<00:00, 38.13batch/s, loss=0.023]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0412, Accuracy: 9864/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21: 100%|██████████| 938/938 [00:27<00:00, 34.70batch/s, loss=0.0222] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 9858/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22: 100%|██████████| 938/938 [00:25<00:00, 37.15batch/s, loss=0.0194] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0447, Accuracy: 9851/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23: 100%|██████████| 938/938 [00:25<00:00, 37.50batch/s, loss=0.0213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0394, Accuracy: 9870/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24: 100%|██████████| 938/938 [00:25<00:00, 37.50batch/s, loss=0.199]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0503, Accuracy: 9820/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25: 100%|██████████| 938/938 [00:24<00:00, 37.79batch/s, loss=0.011]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0406, Accuracy: 9863/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hyperparameters['lr'])\n",
    "\n",
    "for epoch in range(1, hyperparameters['epochs'] + 1):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Optimized(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Optimized, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(16, 32, kernel_size=(4, 4), stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32*3*3, 10)\n",
    "    \n",
    "    def forward(self, x): # (batch_size, channels, height, width)\n",
    "        G_conv_1 = x.flatten().size(dim=0)\n",
    "        x = self.conv_1(x) # [64, 1, 28, 28] -> [64, 16, 28, 28]\n",
    "        G_conv_1 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        G_max_pool_1 = x.flatten().size(dim=0)\n",
    "        x = F.max_pool2d(x, kernel_size=(4, 4)) # [64, 16, 28, 28] -> [64, 16, 7, 7]\n",
    "        G_max_pool_1 /= x.flatten().size(dim=0)\n",
    "\n",
    "        G_conv2 = x.flatten().size(dim=0)\n",
    "        x = self.conv_2(x) # [64, 16, 7, 7] -> [64, 32, 6, 6]\n",
    "        G_conv2 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        G_max_pool_2 = x.flatten().size(dim=0)\n",
    "        x = F.max_pool2d(x, kernel_size=(2, 2)) # [64, 32, 6, 6] -> [64, 32, 3, 3]\n",
    "        G_max_pool_2 /= x.flatten().size(dim=0)\n",
    "\n",
    "        x = x.view(-1, 32*3*3)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # print(\"-------------------\")\n",
    "        # print(f\"G_conv_1: {G_conv_1}\")\n",
    "        # print(f\"G_max_pool_1: {G_max_pool_1}\")\n",
    "        # print(f\"G_conv2: {G_conv2}\")\n",
    "        # print(f\"G_max_pool_2: {G_max_pool_2}\")\n",
    "        # print(\"-------------------\")\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1: 100%|██████████| 938/938 [00:13<00:00, 69.21batch/s, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4338, Accuracy: 8756/10000 (88%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2: 100%|██████████| 938/938 [00:13<00:00, 67.91batch/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2641, Accuracy: 9235/10000 (92%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3: 100%|██████████| 938/938 [00:13<00:00, 71.01batch/s, loss=0.374] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2376, Accuracy: 9273/10000 (93%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4: 100%|██████████| 938/938 [00:13<00:00, 71.72batch/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1767, Accuracy: 9455/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5: 100%|██████████| 938/938 [00:12<00:00, 72.18batch/s, loss=0.274] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1604, Accuracy: 9501/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6: 100%|██████████| 938/938 [00:13<00:00, 72.04batch/s, loss=0.0474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1299, Accuracy: 9614/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7: 100%|██████████| 938/938 [00:13<00:00, 71.91batch/s, loss=0.0782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 9668/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8: 100%|██████████| 938/938 [00:12<00:00, 74.12batch/s, loss=0.0391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1030, Accuracy: 9678/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9: 100%|██████████| 938/938 [00:12<00:00, 75.23batch/s, loss=0.181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1101, Accuracy: 9667/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10: 100%|██████████| 938/938 [00:12<00:00, 75.85batch/s, loss=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0906, Accuracy: 9726/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11: 100%|██████████| 938/938 [00:12<00:00, 74.51batch/s, loss=0.0326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0971, Accuracy: 9678/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12: 100%|██████████| 938/938 [00:12<00:00, 72.40batch/s, loss=0.0157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0787, Accuracy: 9750/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13: 100%|██████████| 938/938 [00:12<00:00, 72.69batch/s, loss=0.0877] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0792, Accuracy: 9731/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14: 100%|██████████| 938/938 [00:13<00:00, 71.62batch/s, loss=0.124]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0700, Accuracy: 9782/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15: 100%|██████████| 938/938 [00:14<00:00, 66.06batch/s, loss=0.112]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0756, Accuracy: 9755/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16: 100%|██████████| 938/938 [00:14<00:00, 65.87batch/s, loss=0.113]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0668, Accuracy: 9790/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17: 100%|██████████| 938/938 [00:12<00:00, 73.26batch/s, loss=0.0329] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0641, Accuracy: 9800/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18: 100%|██████████| 938/938 [00:12<00:00, 74.32batch/s, loss=0.322]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0762, Accuracy: 9753/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19: 100%|██████████| 938/938 [00:12<00:00, 74.46batch/s, loss=0.0299] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0593, Accuracy: 9806/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20: 100%|██████████| 938/938 [00:12<00:00, 75.36batch/s, loss=0.03]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 9823/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21: 100%|██████████| 938/938 [00:13<00:00, 70.16batch/s, loss=0.131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9813/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22: 100%|██████████| 938/938 [00:13<00:00, 71.46batch/s, loss=0.0925] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0595, Accuracy: 9801/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23: 100%|██████████| 938/938 [00:12<00:00, 72.59batch/s, loss=0.109]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0548, Accuracy: 9823/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24: 100%|██████████| 938/938 [00:13<00:00, 71.81batch/s, loss=0.00958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0526, Accuracy: 9830/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25: 100%|██████████| 938/938 [00:12<00:00, 72.96batch/s, loss=0.0105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0565, Accuracy: 9832/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_optimized = CNN_Optimized().to(device)\n",
    "optimizer_optimized = torch.optim.SGD(model_optimized.parameters(), lr=hyperparameters['lr'])\n",
    "\n",
    "for epoch in range(1, hyperparameters['epochs'] + 1):\n",
    "    train(model_optimized, device, train_dataloader, optimizer_optimized, epoch)\n",
    "    test(model_optimized, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 196.04 KB\n",
      "Optimized model size: 44.04 KB\n",
      "Compression ratio: 4.45\n"
     ]
    }
   ],
   "source": [
    "def calculate_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_kb = (param_size + buffer_size) / 1024\n",
    "    return size_kb\n",
    "\n",
    "original_size_kb = calculate_model_size(model)\n",
    "optimized_size_kb = calculate_model_size(model_optimized)\n",
    "\n",
    "print(f\"Original model size: {str.format('{0:.2f}', original_size_kb)} KB\")\n",
    "print(f\"Optimized model size: {str.format('{0:.2f}', optimized_size_kb)} KB\")\n",
    "print(f\"Compression ratio: {str.format('{0:.2f}', original_size_kb / optimized_size_kb)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Blind Experimentation\n",
    "For the first part of this experiment, I implemented a simple CNN to classify MNIST digits. After some blind testing of different architectures, I arrived at a model with 2 convolutional layers and 1 fully-connected decision layer (see below architecture 1 summary). Each convolutional layer also had ReLU activation and max pooling with a $2$ x $2$ kernel and stride of 1 in both dimensions. For simplicity of the experiment, no learning rate scheduling or dropout was used.\n",
    "\n",
    "### Architecture 1\n",
    "```\n",
    "CNN(\n",
    "  (conv_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (conv_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "With a training batch size of $64$ and learning rate of $0.01$, this model achieved $98.63\\%$ accuracy on the test set after $25$ epochs. \n",
    "\n",
    "### Analysis of Architecture 1\n",
    "I first measured the compression ratio of each convolutional layer in the model:\n",
    "\n",
    "```\n",
    "Compression ratio of conv_1: 0.03125  (1:32 ratio)\n",
    "Compression ratio of max_pool_1: 4.0  (4:1 ratio)\n",
    "Compression ratio of conv_2: 0.5      (1:2 ratio)\n",
    "Compression ratio of max_pool_2: 4.0  (4:1 ratio)\n",
    "```\n",
    "\n",
    "Per the multiplicativity of generalization, the total compression ratio of the two convolutional layers if $G_{conv} = 0.03125 * 4.0 * 0.5 * 4.0 = 0.25$. This means that the model is expanding the inputs by a factor of $4$ before feeding them into the decision layer. Now, the decision layer has $10$ neurons, each with a bias parameter. Each of these neurons receives at most $1$ bit of information from each of the $3136$ features resulting from the second $2$ x $2$ max pooling operation. This results in an overall MEC of $(3136 + 1) * 10 = 31370$ bits. The two convolutional layers have \"compressed\" the (maximally) $28^2 * \\log_2(0.5) = -784$ bits of information in the input image by a factor of $0.25$, resulting in $784 * 0.25 = 196$ bits of information received by the decision layer. This means that the decision layer is overparameterized by a factor of approximately $160$. This implies that we can reduce have significant room to shrink the model without sacrificing performance, which is done in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Applying Measurements\n",
    "For the second part of this experiment, I used the above analysis of the MEC of the decision layer to reduce the size of the model. With the goal of increasing the compression ratio of each convolutional layer to reduce the MEC of the decision layer, this included:\n",
    "\n",
    "- reducing the number of output channels of the first convolutional layer from $32$ to $16$\n",
    "- increasing the kernel size of the second convolutional layer from $3$ x $3$ to $4$ x $4$\n",
    "- increasing the kernel size of the second max pooling layer from $2$ x $2$ to $4$ x $4$\n",
    "\n",
    "With the same hyperparameters used in Part 1, this model achieved $98.32\\%$ accuracy on the test set after $25$ epochs.\n",
    "\n",
    "\n",
    "### Architecture 2\n",
    "```\n",
    "CNN_Optimized(\n",
    "  (conv_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (conv_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
    "  (fc): Linear(in_features=288, out_features=10, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "### Analysis of Architecture 2\n",
    "\n",
    "```\n",
    "Compression ratio of conv_1: 0.0625     (1:16 ratio)\n",
    "Compression ratio of max_pool_1: 16.0   (16:1 ratio)\n",
    "Compression ratio of conv_2: 0.68056    (1:1.47 ratio)  \n",
    "Compression ratio of max_pool_2: 4.0    (4:1 ratio)\n",
    "```\n",
    "\n",
    "The total compression ratio of the convolutional layers in this new model is $G_{conv} = 0.0625 * 16.0 * 0.68056 * 4.0 = 2.722$, which is over $10$ times more compressive than the convolutional layers in the first model. Accordingly, the MEC of the decision layer is reduced by over $10\\text{x}$: $(288 + 1) * 10 = 2880$ bits. Since this is still sufficiently larger than the $196$ bits of information in the input image, we still easily achieve the same level of performance with a much smaller model.\n",
    "\n",
    "### Practical Implications:\n",
    "##### Model Size\n",
    "The new model is $4.45$ times smaller than the original model at $44.04$ KB compared to the original $196.04$ KB.\n",
    "\n",
    "##### Training Time\n",
    "The new model also trains $1.89$ times faster, taking $5:47$ s to train for $25$ epochs compared to the original model's $10:56$ s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eml_midterm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
